---
title: "Pratical Machine Learning write-up Project"
author: "lingyu hu"
date: "October 21, 2015"
output: html_document
keep_md: true
---
### Background Introduction
This is the R markdown file of the course project for the Pritical Machine Learning class. Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement ??? a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.In this project,using the data data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants,a model is bulit to predict the manner in which they did the exercise.

###Data Sources
The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project comes from this original source: http://groupware.les.inf.puc-rio.br/har.
### Load the required packages
```{r}
library(rpart)
library(caret)
library(rattle)
library(randomForest)
set.seed(12345)
```
### Download and read the files
```{r}
trainurl<-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testurl<-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(trainurl,"~/coursera/train1.csv",method="curl")
download.file(testurl,"~/coursera/test1.csv",method="curl")
training<-read.csv("~/coursera/train1.csv")
testing<-read.csv("~/coursera/test1.csv")
```
###Partioning the training into testing and training sets
```{r}
intrain<-createDataPartition(training$classe,p=0.6,list=FALSE)
mytraining<-training[intrain,]
mytesting<-training[-intrain,]
```
### Remove the data with near zero variance
```{r}
nzv<-nearZeroVar(mytraining,saveMetrics=TRUE)
zerovar<-subset(nzv,nzv==TRUE)
zvnames<-rownames(zerovar)
zvar<-names(mytraining) %in% c(zvnames)
mytraining<-mytraining[!zvar]
mytraining<-mytraining[,-1]
dim(mytraining)
```
### Clean the data with too many NAs
There are some variables with NA datas, which will cause problems in the predictions we are going to take, so we remove the varibles which have more than 60% NA datas. 
```{r}
isnas<-sapply(mytraining,is.na)
nanum<-colMeans(isnas)
istrue<-nanum>0.6
toonas<-nanum[istrue==TRUE]
nasname<-names(mytraining) %in% c(names(toonas))
mytraining<-mytraining[!nasname]
dim(mytraining)
```
### Clean the data in mytesting and testing sets
To do the prediction, we clean the mytesting and testing data as we did to the mytraining data.
```{r}
clean1<-colnames(mytraining)
mytesting<-mytesting[clean1]
dim(mytesting)

clean2<-colnames(mytraining[,-58])
testing<-testing[clean2]
dim(testing)
```
To make sure that the class of testing data is the same of mytraining data, we use the sample method to solve the problem.
```{r}

testing <- rbind(mytraining[7, -58] , testing)
testing<-testing[-1,]
```
### Use the tree plot
```{r}
modelrpart<-rpart(classe~.,method="class",data=mytraining)
fancyRpartPlot(modelrpart)
```
### Do the prediction in mytesting
```{r}
prerpart<-predict(modelrpart,mytesting,type="class")
confusionMatrix(prerpart,mytesting$classe)
```
### Using ML algorithms for prediction: Random Forests
```{r}
modelrf<-randomForest(classe~.,data=mytraining)
prerf<-predict(modelrf,mytesting)
confusionMatrix(prerf,mytesting$classe)
```
### Conclusion
From the accurancy we can get the conclusion that the randomforest get the better results.so we use the randomforset.

___________________________
### The Final Prediction
Run the randomforset to predict the testing set
```{r}
prefinal<-predict(modelrf,testing)
prefinal
```